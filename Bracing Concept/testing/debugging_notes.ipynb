{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a95b96",
   "metadata": {},
   "source": [
    "# References (& Notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60643238",
   "metadata": {},
   "source": [
    "The `find_peaks` function from `scipy` has an attribute called `prominence` which was a new concept for me in signal processing.\n",
    "\n",
    "The essence of it is (centred around different local maxima of a function) and sees the distance from the two tangents (that are slightly offset from the peaks) to their lowest point. That distance in amplitude indicates the prominence of the signal and is used in signal processing to *smooth-off* noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490bbdb0",
   "metadata": {},
   "source": [
    "Lines 122-132:\n",
    "\n",
    "Below is the code excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "elif (time.time() - poor_posture_start_time) > threshold_time:\n",
    "    \"\"\"\n",
    "    There is now sustained poor posture, therefore model a distribution of the deviances of the posture. Depending on the threshold time, I could include a recency bias.\n",
    "    Send a signal to serial indicating the position of this poor posture and where the average of this is.\n",
    "    The signal processing (a dedicated function for ease) will send out:\n",
    "        1. The sensor(s) which is/are nearest to the deviations\n",
    "        2. The intensity of the haptic motor signal (from the distribution of peaks (should use deques for this aspect with the recency bias))\n",
    "        3. Tackle the issue of n-number deviances\n",
    "        4. Share this to the serial in this format: \"serial.write(bf'Curve, {signal_response[0]}, {signal_response[1]}')\"\n",
    "        5. Use the indices to plot the points in 3D space\n",
    "    \"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8861b",
   "metadata": {},
   "source": [
    "It has been taking me much longer to solve this issue that I have since there are multiple overlapping requirements that need to be handled by some helper functions. Therefore, I will take the time to explain my thinking process for this and include links to subsequent files in this repo the explores the derivation for this solution in more depth.\n",
    "\n",
    "#### Rolling Mean\n",
    "For the duration of the *deviation* there will be a distribution of different functions (deviation against postition along spine graph) for each instance of this. To aggregate this, I would like to make an algorithm to make a mean that can account for anomalies (I don't think any of the measures of central tendency accounts for this specific goal that I have).\n",
    "\n",
    "Below will explore this clearer:\n",
    "$$\n",
    "\\mathrm{devation\\_array\\_0} = \\begin{bmatrix}i_0 & i_1 & i_2 & ... & i_n\\end{bmatrix} \\\\\\\n",
    "\n",
    "\\\\\n",
    "\\mathrm{devation\\_array\\_1} = \\begin{bmatrix}j_0 & j_1 & j_2 & ... & j_n\\end{bmatrix} \\\\\\\n",
    "\n",
    "\\\\\n",
    "... \\\\\\\n",
    "\n",
    "\\\\\n",
    "\\mathrm{devation\\_array\\_n} = \\begin{bmatrix}k_0 & k_1 & k_2 & ... & k_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For each column in this large matrix, I would like to create an average of the results to output a final array which can be approximated by a single function:$\\mathrm{aggregated\\_devation\\_array} = \\begin{bmatrix}\\lambda_0 & \\lambda_1 & \\lambda_2 & ... & \\lambda_n\\end{bmatrix}$, which approximates to $f(x)$.\n",
    "\n",
    "However, if there were anomalous extremes in this (the aggregation of noise in the sensors for examples or just a random spike) I wouldn't want this affecting, drastically, the distribution of the points and deviation.\n",
    "\n",
    "Therefore, this algorithm will occur with an example:\n",
    "\n",
    "Let's define the list:\n",
    "$$\\gamma = [3,5,6,9,13,15,1700]$$\n",
    "The arithmetic mean of this distribution is: $$\\frac{1751}{7} \\approx 250$$\n",
    "If using groups of length 4 however, the result differ: $$\\frac{\\frac{3+5+6+9}{4} + \\frac{5+6+9+13}{4} + \\frac{6+9+13+15}{4} + \\frac{9+13+15+1700}{4}}{4}\\\\\\\n",
    "\\\\=\\frac{459}{4} \\approx 115$$\n",
    "\n",
    "I am not sure why this result arises but trying to test and define this algorithm will help with my application in code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02b0af",
   "metadata": {},
   "source": [
    "A better approach for summarising the values of the $n$-th index could be centering the values around the median value and then finding the interquartile mean. From this, it would help to remove the anomalous values: however, through the testing below it could be shown that this approach actually removes important areas (higher values than the mean doesn't always mean anomalous!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "List Setup\n",
    "\"\"\"\n",
    "\n",
    "# use a random list of varying degrees as the base test\n",
    "array_1 = np.random.uniform(0, 100, 200)\n",
    "array_1 = np.concatenate([array_1, np.random.uniform(0,100,5)])\n",
    "\n",
    "# then use a similar list but with a very large values placed as anomalies\n",
    "anomalies = np.random.uniform(10000,25000,5)\n",
    "array_2 = np.concatenate([array_1, anomalies])\n",
    "\n",
    "# finally use a fluctuating list\n",
    "\n",
    "# try to plot each of these values against each other and choose the best appraoch to this (with my data)\n",
    "\n",
    "def listing_method(n, list_input):\n",
    "    \"\"\"\n",
    "    There is an error that if I input a value of \"n\" larger than len(list_input) after one error has occurred, it will just default to time_series_mean == 0.000\n",
    "    I do not know how to to debug that.\n",
    "    \"\"\"\n",
    "    setup = True\n",
    "    while setup:\n",
    "        if n > len(list_input) or n < 1 or not isinstance(n, int):\n",
    "            try:\n",
    "                n = int(input(f\"Enter a value for n in range 1 and {len(list_input)}: \"))\n",
    "            except ValueError:\n",
    "                print(\"Please enter an integer.\")\n",
    "        else:\n",
    "            setup = False  # exit only when n is valid\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Input the algorithm for this.\n",
    "    \"\"\"\n",
    "    list_input = np.array(list_input)\n",
    "    subdivided_lists = [list_input[i:(i+n)] for i in range((len(list_input)) - n + 1)]\n",
    "    subdivided_list_mean = [sum(subdivided_lists[i])/n for i in range(len(subdivided_lists))]\n",
    "    time_series_mean = np.mean(subdivided_list_mean)\n",
    "    return time_series_mean\n",
    "\n",
    "def interquartile_mean_calculation(poor_posture_list_input):\n",
    "    list_input = np.array(poor_posture_list_input)\n",
    "    n = len(list_input)\n",
    "\n",
    "    if n == 0:\n",
    "        return np.nan # an empty list\n",
    "    if n < 4: # not enough elements for any meaningful IQR\n",
    "        return np.mean(list_input)\n",
    "\n",
    "    list_input.sort()\n",
    "    n_upper = int(n * 0.75)\n",
    "    n_lower = int(n * 0.25)\n",
    "\n",
    "    interquartile_list = list_input[n_lower:n_upper]\n",
    "\n",
    "    if interquartile_list.size == 0: # incase an unexpected error occurred\n",
    "        return np.mean(list_input)\n",
    "\n",
    "    interquartile_mean = np.mean(interquartile_list)\n",
    "    return interquartile_mean\n",
    "\n",
    "n = 15  # how can I numerically find the best value for n, given the list size?\n",
    "i = listing_method(n, array_1)\n",
    "j = listing_method(n, array_2)\n",
    "k = interquartile_mean(array_1)\n",
    "p = interquartile_mean(array_2)\n",
    "\n",
    "med1 = np.median(array_1)\n",
    "med2 = np.median(array_2)\n",
    "\n",
    "print(f\"listing, array_1: {i}\")\n",
    "print(f\"listing, array_2: {j}\")\n",
    "print(f\"IQM, array_1: {k}\")\n",
    "print(f\"IQM, array_2: {p}\")\n",
    "print(f\"Median, array_1: {med1}\\nMedian, array_2: {med2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be3809",
   "metadata": {},
   "source": [
    "From running this, the interquartile method is significantly better since the runtime is much, much lower (and this will have to run in real time).\n",
    "\n",
    "I will look into ways of optimising this algorithm but since it is dependent on length of the list (of size n) then accuracy will be lost through this process.\n",
    "\n",
    "I'll try to run other similar methods as well to check but the interquartile method is the best (and I will adjust the size of that window (10-90 or 40-60 depending on what I am looking for) in the actual code).\n",
    "\n",
    "I am now interested in how I can find the optimal value for this (if I run some tests) to see what would actually be the best window size - if such a thing exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lis = [9,9,9,8,8]\n",
    "l = np.array(lis)\n",
    "q = np.array(l)\n",
    "print(l)\n",
    "print(q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19fbf6",
   "metadata": {},
   "source": [
    "Curvature test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f480100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cubic spline test: here is a test for the cubic spline calculation since my previous endeavours were actually quite messy (and unhelpful).\n",
    "I will now apply this to the IMU sensor test.\n",
    "\"\"\"\n",
    "from scipy.interpolate import CubicSpline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig= plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "\n",
    "a = (5,4,2)\n",
    "b = (6,7,5)\n",
    "c = (7,9,6)\n",
    "array = np.array([np.array(i) for i in (a,b,c)])\n",
    "\n",
    "t = [0]\n",
    "c_dist = [0]\n",
    "d_dist = 0\n",
    "for i in range(len(array)):\n",
    "    if i == len(array)-1:\n",
    "        break\n",
    "\n",
    "    distance = np.linalg.norm(array[i+1]-array[i])\n",
    "    c_dist.append(distance)\n",
    "    t.append(sum(c_dist))\n",
    "\n",
    "x = array[:, 0]\n",
    "y = array[:, 1]\n",
    "z = array[:, 2]\n",
    "\n",
    "xc = CubicSpline(t,x)\n",
    "yc = CubicSpline(t,y)\n",
    "zc = CubicSpline(t,z)\n",
    "\n",
    "plotting_t = np.linspace(min(t), max(t), 1000)\n",
    "\n",
    "ax.plot(xc(plotting_t), yc(plotting_t), zc(plotting_t))\n",
    "\n",
    "ax.set_xlim(min(xc(plotting_t)), max(xc(plotting_t)))\n",
    "ax.set_ylim(min(yc(plotting_t)), max(yc(plotting_t)))\n",
    "ax.set_zlim(min(zc(plotting_t)), max(zc(plotting_t)))\n",
    "ax.grid()\n",
    "\n",
    "\"\"\"\n",
    "Curvature Calculations\n",
    "\"\"\"\n",
    "curvature_list = []\n",
    "for i in range(len(plotting_t)):\n",
    "    r = (xc(plotting_t[i], 0), yc(plotting_t[i], 0), zc(plotting_t[i], 0))\n",
    "    r_prime = (xc(plotting_t[i], 1), yc(plotting_t[i], 1), zc(plotting_t[i], 1))\n",
    "    r_double_prime = (xc(plotting_t[i], 2), yc(plotting_t[i], 2), zc(plotting_t[i], 2))\n",
    "\n",
    "    kappa = (np.linalg.norm(np.cross(r_prime, r_double_prime)))/(np.linalg.norm(r_prime)**3)\n",
    "    curvature_list.append(kappa)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(plotting_t, curvature_list)\n",
    "ax2.grid()\n",
    "ax2.set_aspect(250)\n",
    "\n",
    "ax.set_title(\"Parametric Curve\")\n",
    "ax2.set_title(r\"d$\\kappa$/dt\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51814e0d",
   "metadata": {},
   "source": [
    "**Curvature Derivation & Practice Questions**:\n",
    "\n",
    "I want to intuitively explain what this means (understanding helps with adding additional features) so I will copy up workings out into LaTeX, make apparent any errors in my process and then apply this to the IMU code as normal.\n",
    "\n",
    "In this derivation, review circular motion and see how it relates to this (and do practice questions on it since my physics is worsening). Try out some PAT, TMUA and MAT papers.\n",
    "\n",
    "Once this is done, the coded aspect of my project is entirely finished off!!\n",
    "\n",
    "----------\n",
    "\n",
    "**Curvature Application**: Here is how I would like to apply this (*eventually it will just be machine learning but we have not gotten there yet*):\n",
    "\n",
    "- There will be a list of the instaneous curvatures along the spline\n",
    "- There will be a storage of how those curvatures change wrt time\n",
    "- There will be a maximum (that is tracked (and maybe marked with a red dot))\n",
    "- There may be a measure of the standard deviation of points\n",
    "- There may be an average (mean/median)\n",
    "\n",
    "*For the initial application, I will be using max($\\kappa$) as it is easiest (I hope) to get working. Then, I will try to develop a pseudocode and apply updates incrementally that works towards the main goal (as written below)*:\n",
    "\n",
    "    \n",
    "    When a user wears the posture bracing, there will be a calibration phase at the beginning where there is an implied best posture. They will then do a range of exercises that will help the model \"understand\" what the user's spine shape is. Then, the threshold is set on the deviance away from this ideal shape.\n",
    "\n",
    "To scaffold onto this ideal, I think it will be best to go as follows:\n",
    "\n",
    "1. max($\\kappa$)\n",
    "2. how $\\kappa$ changes with time; larger differences indicates big curves (if the change results in a larger value -- *this will be difficult code to make robust so maybe this won't be second*)\n",
    "3. integrating across the whole spine and then dividing by the change in bounds\n",
    "    \n",
    "\n",
    "**Derivation**:\n",
    "https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/curvature/v/curvature-formula-part-5\n",
    "\n",
    "When trying to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7471ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "deck = deque(maxlen=10)\n",
    "deck.extend([3, 9, 9, 190])\n",
    "d = np.mean(deck)\n",
    "e = np.median(deck)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e4fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "print(t)\n",
    "time.sleep(2)\n",
    "for i in range(8):\n",
    "    for i in range(9):\n",
    "        q = time.time()\n",
    "        time.sleep(1.2)\n",
    "        z = time.time()\n",
    "        print(q - z)\n",
    "        print(q - t)\n",
    "\n",
    "print(t)\n",
    "print(q)\n",
    "print(t-q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dd1db4",
   "metadata": {},
   "source": [
    "Below is the error in the code:\n",
    "\n",
    "\n",
    "    ```python\n",
    "        if IMU_ID == 1:\n",
    "            IMU_DEQUES[0].append(IMU_DATA_NORM)\n",
    "\n",
    "        elif IMU_ID == 2:\n",
    "            IMU_DEQUES[1].append(IMU_DATA_NORM)\n",
    "\n",
    "        elif IMU_ID == 7:\n",
    "            IMU_DEQUES[2].append(IMU_DATA_NORM)\n",
    "    ```\n",
    "\n",
    "Which means that this cannot be generalised.\n",
    "\n",
    "All I need to do is export, through the serial and handle it, which sensor IDs are successful and place them into a list such that the improved one will be:\n",
    "\n",
    "    ```python\n",
    "    try:\n",
    "        target_IMU_index = IMU_ID_LIST.index(IMU_ID)\n",
    "        IMU_DEQUES[target_IMU_index].append(IMU_DATA_NORM)\n",
    "\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Recieved data from unknown data channel @ ID:{IMU_ID}\")\n",
    "\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
